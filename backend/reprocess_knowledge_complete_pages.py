#!/usr/bin/env python3
"""
Reprocess knowledge_complete pages to rebuild Neo4j graph
"""
import asyncio
import asyncpg
import redis.asyncio as redis
import os

async def main():
    # Connect to Postgres
    pool = await asyncpg.create_pool(
        host=os.getenv('POSTGRES_HOST', 'postgres'),
        port=int(os.getenv('POSTGRES_PORT', 5432)),
        user=os.getenv('POSTGRES_USER', 'herenews_user'),
        password=os.getenv('POSTGRES_PASSWORD', 'herenews_pass'),
        database=os.getenv('POSTGRES_DB', 'herenews')
    )

    # Connect to Redis
    r = await redis.from_url(os.getenv('REDIS_URL', 'redis://redis:6379'))

    # Get knowledge_complete pages
    async with pool.acquire() as conn:
        pages = await conn.fetch("""
            SELECT id, url, word_count
            FROM core.pages
            WHERE status = 'knowledge_complete'
            ORDER BY id
        """)

    print(f"üîÑ Found {len(pages)} knowledge_complete pages to reprocess\n")

    # Reset status to 'extracted' and queue to knowledge worker
    for page in pages:
        page_id = page['id']
        url = page['url']
        word_count = page['word_count']

        # Reset status
        async with pool.acquire() as conn:
            await conn.execute("""
                UPDATE core.pages
                SET status = 'extracted'
                WHERE id = $1
            """, page_id)

        # Queue to knowledge worker
        import json
        job = json.dumps({'page_id': page_id})
        await r.rpush('queue:semantic:high', job)

        print(f"‚úÖ {page_id} ({word_count} words) - {url[:60]}...")

    print(f"\nüöÄ Queued {len(pages)} pages to knowledge worker")
    print("üìã Pipeline: knowledge ‚Üí event")
    print("\n‚è±Ô∏è  Expected time: ~3-5 minutes for knowledge processing")

    await pool.close()
    await r.aclose()

if __name__ == '__main__':
    asyncio.run(main())
