# 32 — Event Formation Mechanism

## The Louvre Heist Case Study

### Progressive Data Accumulation Timeline

```
DAY 1 (Oct 19, 2025) - INITIAL EVENT
├─ Real-world event occurs: Heist at 9:30 AM
├─ Story #1 created: "Heist at the Louvre: €88 Million in Jewelry Stolen"
│   └─ Ingestion pipeline runs:
│       ├─ Fetch 8-10 news articles from various sources
│       ├─ Extract entities: Louvre, Paris, Laure Beccuau, Laurence des Cars...
│       ├─ Create Page nodes → Extract Claims (facts) from each article
│       ├─ Link entities via [:MENTIONS], [:MENTIONS_ORG], [:MENTIONS_LOCATION]
│       └─ Result: 1 story, ~8 pages, ~35 claims, ~20 entities
│
│   Database state after Day 1:
│   ┌─────────────────────────────────────────┐
│   │ Event: "Louvre Heist" (implicit)        │
│   │ └─ Story: "Heist at the Louvre"         │
│   │    ├─ 8 Pages (BBC, Reuters, etc.)      │
│   │    ├─ 35 Claims                          │
│   │    └─ 20 Entities (11 people, 7 orgs,   │
│   │       2 locations)                       │
│   └─────────────────────────────────────────┘

WEEK 2 (Oct 26) - FIRST DEVELOPMENT
├─ Sub-event: Two arrests made
├─ Story #1 updated OR new micro-story created
│   └─ Incremental ingestion:
│       ├─ Fetch 3-5 new articles about arrests
│       ├─ Extract new entities: suspect ages, prior records
│       ├─ Add Claims: "Two suspects arrested", "37-year-old preparing to flee"
│       ├─ Link to existing entities (Louvre, Paris prosecutor)
│       └─ Result: +3 pages, +15 claims, +5 entities
│
│   Database state after Week 2:
│   ┌─────────────────────────────────────────┐
│   │ Event: "Louvre Heist" (implicit)        │
│   │ ├─ Story: "Heist at the Louvre"         │
│   │ │  ├─ 11 Pages                           │
│   │ │  ├─ 50 Claims                          │
│   │ │  └─ 25 Entities                        │
│   │ └─ Timeline enriched with arrest data    │
│   └─────────────────────────────────────────┘

WEEK 5 (Nov 25) - MAJOR DEVELOPMENT
├─ Sub-event: Four more arrests, DNA evidence
├─ Story #2 created: "Four more arrests made in Louvre heist investigation"
│   └─ Full ingestion cycle:
│       ├─ Fetch 4-5 articles from multiple sources
│       ├─ Extract new entities: Laurent Nuñez, additional suspects
│       ├─ Add Claims: "DNA found in truck", "Four arrested", ages, genders
│       ├─ LINK to Story #1 (same event)
│       └─ Result: +2 pages, +13 claims, +7 entities
│
│   Database state after Week 5 (CURRENT):
│   ┌──────────────────────────────────────────────┐
│   │ EVENT: "Louvre Heist 2025" (explicit)        │
│   │ ├─ Story #1: Initial heist (Oct 19)          │
│   │ │  ├─ 8 Pages → 35 claims                    │
│   │ │  └─ 20 entities                             │
│   │ ├─ Story #2: Latest arrests (Nov 25)         │
│   │ │  ├─ 5 Pages → 28 claims                    │
│   │ │  └─ 12 entities (7 new, 5 overlapping)     │
│   │ └─ AGGREGATED EVENT VIEW:                    │
│   │    ├─ 2 stories                               │
│   │    ├─ 13 unique Pages                         │
│   │    ├─ 63 total Claims                         │
│   │    └─ 32 unique Entities (deduplicated)      │
│   │       ├─ 11 People (Macron, Dati, Nunez...)  │
│   │       ├─ 16 Organizations (Louvre, govt...)  │
│   │       └─ 5 Locations (Paris, Galerie...)     │
│   └──────────────────────────────────────────────┘
```

---

## The Scalable Event Formation Mechanism

### 1. **Atomic Layer: Claims as Facts**

Every piece of information is an atomic Claim:
- Text: "Three to four thieves arrived on TMax scooters"
- Confidence: 0.95
- Modality: "reported" (vs "alleged", "confirmed")
- Temporal context: "2025-10-19T09:30:00"
- Event time: When the fact occurred (vs when reported)

**Claims are the building blocks of everything.**

### 2. **Aggregation Layer: Pages → Stories → Events**

```
Claims (atomic facts)
    ↓ extracted from
Pages (source articles)
    ↓ aggregated into
Stories (narrative snapshots at a point in time)
    ↓ linked to form
Events (complete picture across time)
```

**Key insight from `event_page.py:33-42`:**
```python
# Get ALL Louvre-related stories
stories_result = session.run('''
    MATCH (s:Story)
    WHERE toLower(s.title) CONTAINS 'louvre'
       OR toLower(s.summary) CONTAINS 'louvre'
    RETURN s.id, s.title, s.created_at
    ORDER BY s.created_at ASC
''')
```

Stories are discovered via **semantic clustering** (both explicitly in DB and implicitly via query). Multiple stories about "Louvre" are aggregated into one Event.

### 3. **Deduplication Layer: Canonical Entities**

From `event_page.py:61-94`:
```python
# Get ALL entities from all stories
OPTIONAL MATCH (s)-[:MENTIONS]->(person:Person)
OPTIONAL MATCH (s)-[:MENTIONS_ORG]->(org:Organization)
...
RETURN collect(DISTINCT { id: person.canonical_id, ... }) as people
```

- Each entity has `canonical_id` and `canonical_name`
- Multiple mentions across stories deduplicate to single entity
- Example: "Laurent Nunez" in Story #1 + "Minister Nunez" in Story #2 → same `Person` node
- **Result**: 32 unique entities across 2 stories (not 40+ duplicates)

### 4. **Credibility Layer: Source Evaluation**

From `event_page.py:122-131`:
```python
if any(tier1 in domain for tier1 in ['bbc.', 'reuters.', 'nbcnews.']):
    credibility = 95  # tier1
elif any(tier2 in domain for tier2 in ['usatoday.', 'nationalpost.']):
    credibility = 85  # tier2
else:
    credibility = 75  # tier3
```

Each Page gets credibility score based on outlet reputation. This allows:
- Weighing claims by source quality
- Highlighting most credible sources
- Detecting consensus (multiple tier1 sources saying same thing)

### 5. **Synthesis Layer: Timeline + Overview**

This is where human-like understanding emerges:

**Timeline synthesis** (`event_page.py:155-256`):
- 63 raw Claims → 10 coherent timeline events
- Each event has precise timestamp, title, description
- Grouped by significance: critical (heist), high (arrests), medium (statements)
- Verified status based on source consensus

**Overview synthesis** (`event_page.py:265-300`):
- 63 Claims + 32 Entities + 2 Stories → narrative prose
- Four sections: The Heist, Investigation, Arrests, Current Status
- Incorporates contradictions: "Nunez called them 'experienced' while Beccuau said 'petty criminals'"
- Temporal-spatial grounding: "October 19, 9:30 AM", "Galerie d'Apollon", "Bank of France"

---

## Scaling the Mechanism: Micro to Macro

### **MICRO EVENTS** (hours to days)
**Example**: "White House Press Conference"
- Duration: 1 hour
- Stories: 1-2 (initial + follow-up)
- Pages: 5-10 sources
- Claims: 20-40 facts
- Entities: 5-10 (speakers, topics)

**Formation**:
1. Event occurs
2. Single story created within hours
3. Claims extracted from live transcripts + news articles
4. Timeline: minute-by-minute key statements
5. Overview: "President announced..."

### **MESO EVENTS** (days to weeks)
**Example**: "Louvre Heist 2025" (current case)
- Duration: 5+ weeks (ongoing)
- Stories: 2-3 (heist + arrests + trial)
- Pages: 10-15 sources
- Claims: 50-100 facts
- Entities: 30-50 (officials, suspects, locations, artifacts)

**Formation**:
1. Initial event creates Story #1
2. Major developments (arrests) create Story #2, #3
3. Stories linked via shared entities (Louvre, prosecutor)
4. Timeline: daily/weekly key events
5. Overview: "Over five weeks..." with multiple sections

### **MACRO EVENTS** (months to years)
**Example**: "2024 US Presidential Election"
- Duration: 18 months (primaries → election → certification)
- Stories: 100+ (debates, primaries, election night, Jan 6...)
- Pages: 1000+ sources
- Claims: 10,000+ facts
- Entities: 500+ (candidates, states, officials, issues)

**Formation**:
1. Parent event "2024 Election" with sub-events:
   - "Iowa Caucuses"
   - "Super Tuesday"
   - "Presidential Debates"
   - "Election Night"
   - "Certification"
2. Each sub-event has 5-20 stories
3. Stories cluster via temporal + semantic similarity
4. Timeline: major milestones (monthly)
5. Overview: multi-chapter narrative

**Hierarchical structure**:
```
Event: 2024 US Presidential Election
├─ Sub-event: Iowa Caucuses (Jan 15)
│  ├─ Story: "Trump wins Iowa with 51%"
│  ├─ Story: "DeSantis drops out after Iowa"
│  └─ Timeline: 10 events (caucus night)
├─ Sub-event: Super Tuesday (March 5)
│  ├─ Story: "Biden sweeps Super Tuesday"
│  ├─ Story: "Haley exits race"
│  └─ Timeline: 15 events (state-by-state)
└─ Sub-event: Election Night (Nov 5)
   ├─ Story: "Biden re-elected"
   ├─ Story: "Senate flips Republican"
   └─ Timeline: 50 events (hour-by-hour)
```

---

## The Universal Pattern

### Data Flow (Same at All Scales)
```
1. Real-world event occurs
2. News articles published
3. Ingestion pipeline:
   ├─ Fetch articles (Pages)
   ├─ Extract Claims (facts)
   ├─ Extract Entities (people, orgs, places)
   └─ Create/update Story
4. Linking:
   ├─ Deduplicate entities across articles
   ├─ Link stories via shared entities + temporal proximity
   └─ Form Event cluster
5. Synthesis:
   ├─ Build timeline from temporal Claims
   ├─ Generate overview from all Claims
   └─ Calculate credibility/coherence scores
```

### Query Pattern (Same at All Scales)

From `event_page.py:33-42, 61-113`:
```cypher
// 1. Find all stories for this event (by keyword, ID, or graph traversal)
MATCH (s:Story) WHERE toLower(s.title) CONTAINS 'louvre'

// 2. Get all entities across stories (deduplicated)
MATCH (s)-[:MENTIONS*]->(entity)
RETURN collect(DISTINCT entity)

// 3. Get all sources with claims
MATCH (s)-[:HAS_ARTIFACT]->(page:Page)-[:HAS_CLAIM]->(claim:Claim)
RETURN page, collect(claim)

// 4. Build complete event view
RETURN {
  stories: [...],
  entities: [...],
  claims: [...],
  timeline: [...],
  overview: "..."
}
```

### Synthesis Pattern (Scales with LLM Context)

**For micro events**: Single LLM call with all 20-40 claims
**For meso events**: Single LLM call with 50-100 claims (current Louvre case)
**For macro events**: Hierarchical synthesis
1. Synthesize each sub-event (100 claims → 1 paragraph)
2. Synthesize parent event (10 paragraphs → 1 page)
3. Use RAG/agentic retrieval for specific queries

---

## Key Mechanisms That Enable Scaling

### 1. **Temporal Clustering**
- Claims have `event_time` (when fact occurred) vs `created_at` (when ingested)
- Stories within 7 days + shared entities = same Event
- Timeline built from `event_time` sequence across all Claims

### 2. **Entity-Based Linking**
- Two stories mentioning same `canonical_id` entity = related
- Graph traversal: `MATCH (s1:Story)-[:MENTIONS]->(e)<-[:MENTIONS]-(s2:Story)`
- Forms event clusters without explicit event_id

### 3. **Claim Confidence**
- Each Claim has confidence score
- Timeline events weighted by claim confidence
- Overview prioritizes high-confidence claims

### 4. **Incremental Updates**
- New story arrives → update event aggregation
- New claims → refine timeline
- New entities → expand entity graph
- No need to reprocess everything

### 5. **Credibility Propagation**
- Page credibility → Claim credibility → Timeline event credibility
- Consensus detection: same claim from multiple tier1 sources = verified
- Outlier detection: claim only from tier3 source = unverified

---

## What Makes This Louvre Event "Rich and Complete"?

1. **Atomic facts** (63 Claims) not summaries
2. **Temporal precision** (timestamps down to minutes)
3. **Spatial grounding** (5 locations, physical details)
4. **Entity web** (32 actors with Wikidata linkage)
5. **Source transparency** (13 pages, credibility scores)
6. **Contradiction handling** ("experienced" vs "petty criminals")
7. **Synthesis** (raw claims → coherent narrative)
8. **Live updates** (ongoing status, no recovered jewels yet)

**Compare to traditional news**:
- Traditional: One article, one perspective, one snapshot
- Our system: 13 sources, 63 facts, 2 story snapshots, complete timeline, all entities

---

## Universal Epistemic Engine (UEE)

The event formation mechanism is now implemented through the **Universal Epistemic Engine**, which uses a single fractal operation:

```python
affinity = 0.60 × semantic + 0.25 × entity_overlap + 0.15 × entity_specificity
```

### Claim-Level Routing

Instead of routing pages, the UEE routes individual claims:

```
For each claim:
    score = compute_affinity(claim, event)

    if score >= 0.45:
        JOIN event
    else:
        SEED new event
```

**Key advantage**: No orphan claims. In page-level routing, ~50% of claims are lost because they're bundled with unrelated claims. In claim-level routing, each claim finds its best match.

### Event Merging (Same Rules)

The same affinity operation applies to events:

```
For each event pair:
    score = compute_affinity(event_A, event_B)

    if score >= 0.50:
        MERGE B into A
```

This creates **fractal structure**: same rules at claim→event and event→event levels.

### Validated Results

Testing on 1,271 claims:
- **HK Fire**: 140 claims, mass=81.6, 24 sources
- **Jimmy Lai**: 129 claims, mass=77.1, 26 sources
- **Bondi Beach**: 73 claims, mass=48.2, 8 sources

See `docs/31.arch.uee.md` for full specification.

---

## Next Steps for Production

1. **Deploy Claim Pool**: Replace page-level routing with claim-level
2. **Implement UEE Worker**: Use compute_affinity() for routing
3. **Add Jaynesian quantities**: Mass, heat, entropy on events
4. **Display tier ranking**: Surface events by display_score
5. **Cross-event claims**: Enable one claim → multiple events
6. **Economic layer**: Stake/reward for community contributions
7. **Entity bridges**: Surface meta-narratives from shared entities
8. **Hierarchical events**: Sub-event detection via same affinity rules

The mechanism is validated. Now it's about production deployment.
