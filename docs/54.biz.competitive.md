# 54 — Competitive Landscape

This document maps the alternatives to HERE.news and articulates why existing solutions fail the target ICP.

Related:
- `docs/50.biz.plan.md`
- `docs/01.vision.md`

## Target ICP Reminder

**Primary**: Research desks, newsrooms, and analysts who need to track developing situations with:
- Auditable evidence trails
- Change detection and alerts
- Structured timelines with provenance
- Contradiction visibility

**Pain points we address**:
1. "Where did this claim come from?"
2. "What changed since yesterday?"
3. "Which sources disagree and why?"
4. "Can I trust this summary?"

## Competitive Map

### Category 1: News Aggregators

| Competitor | What They Do | Why We're Different |
|------------|--------------|---------------------|
| **Feedly** | RSS aggregation with AI summaries | No claim-level provenance; summaries are black-box; no contradiction handling |
| **Flipboard** | Curated news feed by topic | Consumer-focused; no structured data extraction; no evidence trails |
| **Apple News** | Algorithm-curated news | Engagement-optimized, not truth-optimized; no provenance |
| **Google News** | Search + clustering by topic | Clusters by headline similarity, not by claim-level facts; no evolution tracking |

**Gap they leave**: Aggregators collect articles but don't extract structured knowledge. Users see headlines, not claims. No way to trace "44 dead" back to source or see it evolved from "36 dead."

### Category 2: Media Monitoring / PR Intelligence

| Competitor | What They Do | Why We're Different |
|------------|--------------|---------------------|
| **Meltwater** | Media monitoring, sentiment, reach | Focused on brand mentions and sentiment; not fact extraction; no claim grounding |
| **Cision** | PR distribution + monitoring | Same as Meltwater; optimized for comms teams, not researchers |
| **Brandwatch** | Social listening + analytics | Social-first; no news claim extraction; engagement metrics, not truth metrics |
| **Mention** | Real-time media monitoring | Alerts on keyword mentions; no semantic understanding of claims |

**Gap they leave**: These tools answer "how much coverage did we get?" not "what actually happened?" They track mentions, not facts. No support for evolving truth or contradictions.

### Category 3: OSINT / Intelligence Platforms

| Competitor | What They Do | Why We're Different |
|------------|--------------|---------------------|
| **Palantir** | Enterprise intelligence platform | Powerful but expensive ($500K+); requires custom integration; not journalism-focused |
| **Babel Street** | Multilingual OSINT | Strong on collection; weak on structured claim extraction and provenance |
| **Maltego** | Entity relationship visualization | Manual graph building; no automatic claim extraction; no event formation |
| **Recorded Future** | Threat intelligence | Security-focused; not general news events; no community correction |

**Gap they leave**: OSINT tools are expensive, security-focused, and require significant analyst effort to extract structured knowledge. No automatic claim grounding or community correction mechanisms.

### Category 4: Fact-Checking / Bias Detection

| Competitor | What They Do | Why We're Different |
|------------|--------------|---------------------|
| **Ground News** | Bias rating by outlet; story coverage comparison | Rates outlets, not claims; static ratings, not dynamic reputation; no claim extraction |
| **NewsGuard** | Publisher credibility ratings | Manual ratings (slow to update); outlet-level only, not claim-level |
| **Snopes / PolitiFact** | Manual fact-checking articles | Human-written; doesn't scale; reactive (checks after viral); no systematic coverage |
| **ClaimBuster** | Claim detection for fact-checkers | Detects "check-worthy" claims; doesn't verify or track evolution |

**Gap they leave**: Fact-checking is manual, slow, and reactive. Bias detection is outlet-level, not claim-level. No one tracks how claims evolve across sources or surfaces contradictions automatically.

### Category 5: AI Summarization / Research Tools

| Competitor | What They Do | Why We're Different |
|------------|--------------|---------------------|
| **Perplexity** | AI search with citations | Citations are URL-level, not claim-level; no contradiction handling; no event evolution |
| **ChatGPT + Browse** | Conversational search | Hallucination risk; citations are afterthoughts; no systematic provenance |
| **Elicit / Consensus** | Academic research synthesis | Academic papers, not news; not designed for fast-moving events |
| **Kagi Universal Summarizer** | Quick article summaries | Single-article; no cross-source synthesis; no claim extraction |

**Gap they leave**: AI summarizers generate fluent text but lack systematic grounding. Citations are bolted on, not fundamental. No support for contradictions, updates, or evolving confidence.

## Competitive Positioning Matrix

|  | Claim Extraction | Cross-Source Synthesis | Contradiction Handling | Evolution Tracking | Community Correction |
|--|------------------|------------------------|------------------------|-------------------|---------------------|
| **HERE.news** | ✅ Atomic claims with provenance | ✅ Event formation across sources | ✅ Explicit φ-states | ✅ Update chains, timelines | ✅ Credit-weighted signals |
| Feedly | ❌ Summaries only | ⚠️ Topic clustering | ❌ | ❌ | ❌ |
| Meltwater | ❌ Sentiment, not facts | ⚠️ By outlet | ❌ | ❌ | ❌ |
| Palantir | ⚠️ Custom pipelines | ✅ | ⚠️ Manual | ⚠️ Manual | ❌ |
| Ground News | ❌ Outlet bias only | ⚠️ Story matching | ❌ | ❌ | ❌ |
| Perplexity | ⚠️ URL-level only | ⚠️ Single query | ❌ | ❌ | ❌ |

## Why We Can Win

### 1. Structural Advantage: Claim-Level Provenance

Most competitors stop at the article level. We extract atomic claims with explicit source anchors. This enables:
- "Where did the 44 number come from?" → Click through to source
- Automated contradiction detection
- Confidence scoring based on corroboration

### 2. Temporal Advantage: Evolution Tracking

We treat knowledge as a belief state that changes:
- Update chains: "death toll 5 → 8 → 11"
- "What changed since yesterday?" is a first-class query
- Historical states are preserved, not overwritten

### 3. Epistemic Advantage: Contradiction Handling

Competitors smooth disagreements into a single narrative. We:
- Surface contradictions explicitly (φ± states)
- Allow stable plurality when evidence doesn't converge
- Make "we don't know" an acceptable answer

### 4. Community Advantage: Accountable Correction

Social platforms proved that pure engagement ≠ truth. Our credit + reputation system:
- Gates actions by cost (anti-spam)
- Weights signals by track record (anti-mob)
- Creates audit trails (anti-gaming)
- Allows prior updates without centralized truth authority

### 5. Cost Advantage: AI-Native

Built in 2024/2025 with LLMs as core infrastructure:
- No legacy systems to maintain
- Can leverage model improvements immediately
- Lower headcount vs manual fact-checking organizations

## Investor Questions & Answers

**Q: Why hasn't Meltwater/Cision built this?**
A: Their business model is PR monitoring—tracking brand mentions and sentiment. Fact extraction and truth tracking aren't their product. Retrofitting would require a different data model and UI entirely.

**Q: Why won't Perplexity eat your lunch?**
A: Perplexity optimizes for fluent answers to ad-hoc queries. We optimize for ongoing monitoring with provenance. Different use cases: Perplexity is "search," we're "research infrastructure."

**Q: What's your moat against a well-funded competitor?**
A: Three compounding loops:
1. **Data flywheel**: More sources → better entity resolution → better claim matching → more value per source
2. **Reputation flywheel**: Publisher track records improve claim priors → better confidence → users trust the system → more usage
3. **Community flywheel**: Credits + stakes → quality contributions → better resolution → more trust

**Q: Why now?**
A: LLM extraction quality crossed the threshold in 2023-2024. Previous attempts required expensive NLP pipelines with poor accuracy. Now we can extract structured claims at scale with commodity APIs.

## Market Sizing (TAM/SAM/SOM)

### TAM: Global News Intelligence
- Media monitoring: $5B (Meltwater, Cision, Brandwatch)
- OSINT/Intelligence: $8B (Palantir, Babel Street, Recorded Future)
- Research tools: $2B (Elicit, academic databases)
- **Total TAM**: ~$15B

### SAM: Research-Focused News Intelligence
- Newsrooms and fact-checking: 50,000 organizations × $5K/year = $250M
- Policy/think-tank analysts: 20,000 organizations × $10K/year = $200M
- Risk/compliance teams: 100,000 teams × $15K/year = $1.5B
- **Total SAM**: ~$2B

### SOM: Initial Wedge (Year 1-2)
- 100 design partners at $5K/year average = $500K ARR
- 500 self-serve users at $1K/year average = $500K ARR
- **Target SOM**: $1M ARR

## Differentiation Summary

| Competitor Category | Their Focus | Our Focus |
|---------------------|-------------|-----------|
| Aggregators | Headlines | Claims |
| PR Monitoring | Mentions | Facts |
| OSINT | Collection | Structure |
| Fact-Checkers | Verdicts | Evolution |
| AI Search | Answers | Provenance |

**One-liner**: "Aggregators collect articles. Fact-checkers render verdicts. We track how truth evolves."

Prev: `docs/13.economics.unit-model.md`
Next: `docs/22.product.page-feature-plan.md`
