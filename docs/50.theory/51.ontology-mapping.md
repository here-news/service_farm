# 51 — EVENT Ontology: Jaynesian Probability Mapped to Protocol Mechanics

This document maps E.T. Jaynes' probability theory to the EVENT protocol mechanics.

Related:
- `docs/01.whitepaper.md`
- `docs/13.protocol.event.md`
- `docs/14.epistemic-foundations.metabolism.md`

## Jaynesian Principles → EVENT Design

### Probability as Degree of Plausibility

EVENT treats each claim probability as conditional on available evidence, updated via Bayes' rule rather than votes or frequency counts. The Event container stores P(H|E) and updates as new evidence arrives.

This is Jaynes' core insight: probability is not about frequencies or betting odds—it's about rational degrees of belief given evidence.

### Bayesian Updating as Rational Revision

Claim likelihoods and Event-level posteriors are recomputed with every evidence addition, preserving priors for replay.

```
P(H|E) ∝ P(E|H) · P(H)
```

Where:
- P(H|E) = posterior probability of hypothesis given evidence
- P(E|H) = likelihood of evidence given hypothesis
- P(H) = prior probability of hypothesis

### Maximum Entropy Discipline

Uncertainty is measured through entropy; contributors are rewarded when their evidence materially lowers entropy while maintaining coherence.

When we have no information favoring one hypothesis over another, we assign maximum entropy (uniform distribution). This prevents unjustified certainty.

## Event as Bayesian Belief Container

An **Event** anchors claims to time/place and maintains:

1. **Posterior over competing claims or hypotheses**
   - Multiple claims can coexist with different probabilities
   - No forced binary choice until evidence converges

2. **Log of evidence likelihoods P(E|H)**
   - Every evidence contribution is recorded
   - Enables replay and audit

3. **Entropy Hₙ(E) over the claim set**
   ```
   Hₙ(E) = −Σᵢ P(cᵢ|E) log P(cᵢ|E)
   ```
   - High entropy = high uncertainty = many competing claims
   - Low entropy = convergence toward consensus

4. **Health H(E) capturing coherence**
   ```
   H(E) = 1 − norm(Hₙ(E))
   ```
   - Health rises as entropy falls
   - Represents internal consistency

Story, Narrative, and Resolved State layers aggregate Event-level posteriors without discarding contradictory branches (φ⁰, φ±, φΩ states).

## Entropy, Contradiction, and Coherence

### Entropy (Hₙ(E))

Quantifies uncertainty from contradictory claims. High contradiction → high entropy.

**Example:**
- 3 sources say "23 dead", 2 sources say "40 dead", 1 source says "unknown"
- Entropy is high because probability mass is spread across claims
- As more sources corroborate "23 dead", entropy decreases

### Entropy Delta (Δh)

Captures contribution impact:
```
Δh = H(Eₜ) − H(Eₜ₊₁)
```

Positive Δh means the contribution reduced uncertainty. This is the basis for rewards.

### Coherence

Protocol incentives pay for positive Δh while:
- **Penalizing collusive corroboration** via SSP (Structural Similarity Penalty)
- **Rewarding productive contradiction** via EIR (Entropy-Increasing Reward)

This prevents gaming through coordinated but false evidence.

## Bayesian Flow Across Layers

### 1. Claims → Event

Bayes updates per claim; contradiction retained as rival hypotheses.

When a new claim arrives:
1. Extract likelihood P(E|H) for each existing hypothesis
2. Update posteriors via Bayes' rule
3. Recalculate entropy
4. Log the update for replay

### 2. Event → Story

Correlated Events share posteriors; mutual influence preserves dependency structure.

Events about the same situation (same entities, same timeframe) are not independent. Updates to one Event can propagate to related Events.

### 3. Story → Narrative

Higher-level hypotheses integrate multiple Stories; priors governed via DPGL to avoid oracle bias.

Narrative-level inference asks questions like:
- "What caused this sequence of events?"
- "Is there a pattern across these Stories?"

### 4. Narrative → Resolved State

Settlement occurs when entropy stabilizes:

**φ⁰ (Fully Resolved)**: Entropy ≈ 0, single hypothesis dominates
**φ± (Partially Resolved)**: Entropy stable but non-zero, multiple viable hypotheses
**φΩ (Irreducible)**: Entropy cannot decrease further due to fundamental ambiguity

Fingerprint `φ⁰ = {Σ, Θ, Π, τ}` records:
- Σ = source lattice (who contributed)
- Θ = resolution pattern (how it converged)
- Π = semantic fingerprint (what it means)
- τ = time to convergence

## Temperature as Curiosity

```
T(E) = dH(E)/dt
```

Temperature measures how rapidly entropy is changing:

- **High temperature**: Rapidly changing entropy → system asks more questions, seeks more evidence, runs metabolism more frequently
- **Low temperature**: Stable entropy → system can hibernate, await new evidence

This mimics cognitive curiosity: we pay more attention to situations that are actively evolving.

## Practical Implications

### For Contributors

Your contribution is valuable if it reduces entropy. The system can objectively measure this:
- Submitting corroborating evidence from an independent source: high Δh
- Submitting duplicate evidence from same source: low Δh (SSP penalty)
- Submitting genuine contradiction that clarifies scope: potentially high Δh long-term (EIR reward)

### For Consumers

You can trust the system's confidence levels because they're mathematically grounded:
- Low entropy + high source diversity = high confidence
- High entropy = "we don't know yet" (honest uncertainty)
- φ± state = "multiple valid interpretations" (not a failure)

### For Auditors

Every update is replayable:
- Priors are logged
- Likelihoods are logged
- The posterior follows mechanically
- You can verify any resolution by replaying the evidence stream

## References

- Jaynes, E.T. *Probability Theory: The Logic of Science* (2003)
- Cox, R.T. "Probability, Frequency and Reasonable Expectation" (1946)
- Shannon, C.E. "A Mathematical Theory of Communication" (1948)

Prev: `docs/40.glossary.md`
Next: `docs/50.theory/52.security-model.md`
