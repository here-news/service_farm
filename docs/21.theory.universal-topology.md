# 21 — Universal Claim Topology

> The theoretical spine: why the same operation works everywhere

---

## The Core Insight

**The scientific method IS this model.** Every step of scientific reasoning maps to the same fundamental operation: evidence reducing entropy of claims.

| Scientific Step | Our Model |
|-----------------|-----------|
| Observation | Ground claim (abstraction = 0) |
| Pattern recognition | Claim supported by multiple observations |
| Hypothesis | Higher-order claim with testable predictions |
| Experiment | New evidence that corroborates/contradicts |
| Theory | High-abstraction claim with massive support |
| Paradigm | Frame-level claim organizing theories |

---

## Example: Discovery of Gravity

```
ABSTRACTION 0 (Observations):
├── "Apple fell from tree"
├── "Moon orbits Earth"
├── "Planets orbit Sun"
├── "Tides correlate with Moon position"

ABSTRACTION 1 (Pattern):
└── "Objects with mass attract each other"
    Entropy: reduced by 4 independent observations

ABSTRACTION 2 (Quantified):
└── "F = Gm₁m₂/r²"
    Entropy: reduced by precise predictions matching observations

ABSTRACTION 3 (Deeper):
└── "Spacetime curvature causes gravity"
    Entropy: reduced by Mercury precession, gravitational lensing
```

**Each arrow is the same operation**: evidence reducing entropy of claim.

---

## The Bayesian Foundation

Jaynes' formulation:

```
P(H|E) = P(E|H) × P(H) / P(E)
```

Where:
- `H` = any claim (at any abstraction level)
- `E` = evidence (lower-abstraction claims)

This IS "claim supports claim":

```python
def update(claim_H, evidence_E):
    likelihood = P(E_observed | H_true)
    prior = claim_H.current_probability

    claim_H.probability = likelihood * prior / P(E)
    claim_H.entropy = -p*log(p) - (1-p)*log(1-p)
```

---

## The Calibrated Entropy Formula

Through empirical validation (see `26.theory.calibration.md`), we derived an optimized formula:

```python
def compute_entropy(n_corr, n_contra, independence_ratio=0.35):
    """
    Calibrated entropy formula.

    Validated against LLM assessments.
    MAE = 0.172, Correlation = 0.71
    """
    base = 1.0

    # Effective corroboration (discounted by independence)
    # Key insight: 65% of sources are copies, only 35% are independent
    effective_corr = n_corr * independence_ratio

    # Calibrated weights from optimization
    corr_reduction = 0.49 * (effective_corr ** 0.30)
    contra_addition = 0.27 * (n_contra ** 0.31)

    entropy = base - corr_reduction + contra_addition
    return max(0.05, min(0.99, entropy))
```

### Why Independence Matters

```
100 outlets copying AP ≠ 100 independent sources
```

Empirical finding: **65% of news corroborations are copies**. Naive counting overestimates certainty by ~3x.

The formula discounts by `independence_ratio = 0.35` to account for this.

### Calibration Results

| Claim Type | Average Entropy | Expected |
|------------|-----------------|----------|
| Corroborated | 0.418 | Low ✓ |
| Contested | 0.792 | Medium ✓ |
| Isolated | 1.000 | High ✓ |

Independence amplification correlation = **-1.000** (perfect). More independent sources = lower entropy, exactly as Jaynes predicted.

---

## Universal Claim Topology

### Definition

```python
CLAIM := {
    content: str,           # What is asserted
    entropy: float,         # Uncertainty (0=certain, max=unknown)

    supported_by: [CLAIM],  # Evidence for this claim
    contradicted_by: [CLAIM],

    abstraction_level: computed,  # Graph distance from ground
}

TOPOLOGY := Graph(nodes=CLAIM, edges=SUPPORTS|CONTRADICTS)

INDEX := {
    entropy: uncertainty measure,
    coherence: 1 - normalized_entropy,
    plausibility: P(claim | all evidence)
}
```

---

## Rigorous Examples

### Example 1: Criminal Justice

```
GROUND CLAIMS (abstraction=0):
├── "Fingerprint on weapon" (from forensic report)
├── "A seen near scene at 9pm" (from witness)
├── "A had conflict with victim" (from texts)
├── "A's alibi: was at bar" (from A's statement)
├── "Bar receipt at 9:15pm" (from physical evidence)

PATTERN CLAIM (abstraction=1):
├── "Evidence suggests A was at scene"
│   entropy = f(fingerprint supports, alibi contradicts)
│
└── "A's alibi has partial support"
    entropy = f(receipt supports, but 15min gap)

HIGHER CLAIM (abstraction=2):
└── "A is guilty"
    entropy = f(all evidence, weighted by independence)

    Current state: HIGH ENTROPY (contested)
    - Fingerprint + motive: supports
    - Alibi + receipt: contradicts
    - Needs resolution
```

**The topology shows exactly why we're uncertain and what would resolve it.**

### Example 2: Medical Diagnosis

```
GROUND CLAIMS:
├── "Fever 39°C"
├── "Dry cough"
├── "Fatigue 5 days"
├── "Recent travel to outbreak area"
├── "PCR test positive"

PATTERN CLAIM:
└── "Symptoms match COVID profile"
    entropy: LOW (4/4 symptoms match)

DIAGNOSTIC CLAIM:
└── "Patient has COVID"
    entropy: VERY LOW (symptoms + PCR + exposure)

EPIDEMIOLOGICAL CLAIM:
└── "Community transmission occurring"
    entropy: depends on how many independent cases
```

### Example 3: Historical Knowledge

```
GROUND CLAIMS:
├── "Suetonius wrote Caesar crossed Rubicon" (source: manuscript)
├── "Plutarch wrote same" (source: different manuscript)
├── "Appian wrote same" (source: third manuscript)
├── "No contemporary sources survive"

PATTERN CLAIM:
└── "Multiple ancient historians agree"
    entropy: LOW (3 independent sources)
    BUT: independence questionable (may share lost source)

HISTORICAL CLAIM:
└── "Caesar crossed the Rubicon in 49 BCE"
    entropy: LOW but not zero
    (ancient consensus, no contradicting evidence)
```

### Example 4: Our News Domain

```
GROUND CLAIMS:
├── "Fire reported at Wang Fuk Court 3pm" (RTHK)
├── "17 bodies recovered" (government statement)
├── "Alarms did not sound" (resident interview)
├── "Building passed inspection 2023" (public record)
├── "Jimmy Lai imprisoned 1800 days" (court record)
├── "No trial conclusion yet" (legal filing)

PATTERN CLAIMS:
├── "Deadly fire with safety system failure"
│   entropy: LOW (multiple corroborating sources)
│
└── "Extended detention without trial conclusion"
    entropy: LOW (court records confirm)

FRAME CLAIM:
└── "Hong Kong governance showing systemic failures"
    entropy: MEDIUM
    - Fire response failure: supports
    - Judicial irregularity: supports
    - Independence: HIGH (different domains)
    - Needs more evidence to resolve
```

---

## The Mathematical Core

### Entropy of a Claim

```
H(C) = -Σ P(state_i|evidence) × log P(state_i|evidence)
```

Where:
- `states = {true, false}` for binary claims
- `states = {value₁, value₂, ...}` for multi-valued

### Coherence

```
Coherence(C) = 1 - H(C)/H_max

Where H_max = log(|states|)  # Maximum possible entropy
```

### Plausibility Update (Bayesian)

```
P(C|E_new) = P(E_new|C) × P(C) / P(E_new)

# With independence bonus for compound evidence:
P(C|E₁,E₂) = P(E₁|C) × P(E₂|C) × P(C) / P(E₁,E₂)

# If E₁ ⊥ E₂ (independent):
P(E₁,E₂) = P(E₁) × P(E₂)  # Denominator smaller = stronger update
```

### Abstraction Level (Emergent)

```
level(C) = 0                           if C is ground observation
         = 1 + max(level(s) for s in C.supported_by)  otherwise
```

---

## Why This Is Universal

| Domain | Ground Claims | Pattern Claims | High Claims |
|--------|---------------|----------------|-------------|
| **Physics** | Measurements | Laws | Theories |
| **Medicine** | Symptoms, tests | Syndromes | Diagnoses |
| **Law** | Evidence | Arguments | Verdicts |
| **History** | Sources | Interpretations | Narratives |
| **News** | Reports | Events | Frames |
| **Science** | Observations | Hypotheses | Theories |

**The same topology. The same entropy. The same operation.**

---

## The Universal Engine

If this is right, we have a universal epistemology engine:

```python
class UniversalKnowledge:
    claims: Graph[Claim]

    def add_evidence(self, new_claim: Claim):
        """THE fundamental operation"""
        for existing in self.claims:
            rel = classify(new_claim, existing)

            if rel.supports:
                existing.entropy -= information_gain(new_claim, existing)

            elif rel.contradicts:
                existing.entropy += uncertainty_added
                existing.tension += 1

        # Check for emergent higher-order claims
        for pattern in self.detect_patterns():
            if pattern.support > threshold:
                emergent = self.create_claim(pattern)
                emergent.entropy = compute_from_evidence(pattern)

    def query(self, claim: str) -> ClaimStatus:
        return {
            'entropy': claim.entropy,
            'coherence': 1 - claim.entropy / max_entropy,
            'plausibility': claim.probability,
            'abstraction': claim.level,
            'evidence_chain': claim.trace_to_ground(),
        }
```

---

## Connection to UEE

The Universal Epistemic Engine (see `docs/31.arch.uee.md`) implements this foundation:

| Universal Concept | UEE Implementation |
|-------------------|-------------------|
| Evidence reduces entropy | `compute_affinity()` → JOIN event |
| Same operation at all scales | claim→event, event→event |
| Abstraction emergence | Events emerge from claim clustering |
| Independence bonus | Entity specificity weighting |
| Coherence measure | `coherence = corroborates / total_edges` |
| Entropy tracking | Event entropy over contested values |

The `compute_affinity()` function IS the Bayesian update in disguise:

```python
# Bayesian: P(H|E) ∝ P(E|H) × P(H)
# UEE:      affinity = semantic × entity × specificity

# Both answer: "Given this evidence, how much should we update belief?"
```

---

## The Claim

We can describe any knowledge as:

1. **A topology of claims** (nodes + support/contradict edges)
2. **Each claim has entropy** (uncertainty) and coherence (certainty)
3. **Evidence flows upward** (ground → abstract)
4. **Entropy flows downward** (resolution)
5. **The structure is recursive and scale-free**

This matches:
- ✓ Bayesian epistemology
- ✓ Scientific method
- ✓ Jaynes' probability theory
- ✓ How humans actually reason

---

## Implications

### For HERE.news

We're not building a news aggregator. We're building a **computational implementation of the scientific method** applied to real-time information.

When a user sees an event with:
- `mass = 81.6`
- `coherence = 86%`
- `entropy = 0.12`

They're seeing the same epistemic quantities a scientist sees when evaluating a hypothesis.

### For Knowledge Work

This framework applies to:
- Legal reasoning (evidence → verdict)
- Medical diagnosis (symptoms → diagnosis)
- Scientific research (observations → theory)
- Intelligence analysis (signals → assessment)
- Historical scholarship (sources → narrative)

**The same engine. The same operation. Different domains.**

---

## References

- Jaynes, E.T. *Probability Theory: The Logic of Science* (2003)
- `docs/22.theory.metabolism.md` — Metabolism operations
- `docs/31.arch.uee.md` — UEE implementation
- `docs/23.theory.event-protocol.md` — EVENT protocol specification

---

*The scientific method is not a set of steps. It is a single operation—evidence reducing entropy—applied recursively at all scales. The UEE computes this operation.*
