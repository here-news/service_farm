# 20 — Epistemic Theory

> The theoretical foundation: entropy reduction, universal topology, and metabolism.

---

## Part 1: The Core Truth — Entropy Reduction

**Entropy reduction is the fundamental principle.** Not engagement. Not votes. Not popularity. Not authority. Entropy reduction.

This is not a design choice. It is the recognition that all knowledge systems—from the scientific method to human reasoning to neural networks—operate by the same mechanism: **new evidence reduces uncertainty about claims**.

### The Chain

```
ENTROPY REDUCTION → EFFICIENCY → INTELLIGENCE → CONSCIOUSNESS
```

Biology proves this path:
- Organisms that reduce entropy more efficiently survive
- Efficiency pressure leads to better information processing
- Eventually: nervous systems, brains, consciousness

**For epistemic systems:**

```
EPISTEMIC ENTROPY = disorder in knowledge state
                    (conflicting claims, missing evidence, unresolved contradictions)

METABOLISM       = process of converting external information (claims)
                   into internal order (coherent understanding)

RESOLUTION       = successful entropy reduction (φ⁰ state: resolved)
```

### Why This Is The Right Measure

Traditional information systems optimize for the wrong things:

| System | Optimizes For | Result |
|--------|---------------|--------|
| **Social Media** | Engagement | Amplifies conflict, increases entropy |
| **Legacy News** | Authority | Hides uncertainty, creates false order |
| **Wire Services** | Speed | Sacrifices accuracy for first-mover |
| **Search Engines** | Relevance | Optimizes for query, not truth |
| **Aggregators** | Volume | More sources ≠ better understanding |

**We optimize for entropy reduction:**
- Contradictions are surfaced (honest about disorder)
- Resolution is tracked (progress toward order)
- Uncertainty is explicit (true epistemic state)

### The Thermodynamic Analogy

An epistemic organism is like a biological organism:

| Physical Organism | Epistemic Organism |
|-------------------|-------------------|
| Consumes food (energy) | Consumes claims (information) |
| Maintains internal order | Maintains internal coherence |
| Fights entropy (decay) | Fights entropy (contradiction, noise) |
| Dies without food | Goes dormant without claims |
| Metabolism converts energy | Metabolism converts claims |
| External energy extends life | External funding extends life |

**Key insight**: Just as biological organisms use metabolism to reduce local entropy (while increasing universal entropy), epistemic organisms use metabolism to reduce knowledge entropy (while processing information chaos).

---

## Part 2: Universal Claim Topology

**The scientific method IS this model.** Every step of scientific reasoning maps to the same fundamental operation: evidence reducing entropy of claims.

| Scientific Step | Our Model |
|-----------------|-----------|
| Observation | Ground claim (abstraction = 0) |
| Pattern recognition | Claim supported by multiple observations |
| Hypothesis | Higher-order claim with testable predictions |
| Experiment | New evidence that corroborates/contradicts |
| Theory | High-abstraction claim with massive support |
| Paradigm | Frame-level claim organizing theories |

### The Bayesian Foundation

Jaynes' formulation:

```
P(H|E) = P(E|H) × P(H) / P(E)
```

Where:
- `H` = any claim (at any abstraction level)
- `E` = evidence (lower-abstraction claims)

This IS "claim supports claim":

```python
def update(claim_H, evidence_E):
    likelihood = P(E_observed | H_true)
    prior = claim_H.current_probability

    claim_H.probability = likelihood * prior / P(E)
    claim_H.entropy = -p*log(p) - (1-p)*log(1-p)
```

### The Calibrated Entropy Formula

Through empirical validation (see `26.theory.calibration.md`), we derived an optimized formula:

```python
def compute_entropy(n_corr, n_contra, independence_ratio=0.35):
    """
    Calibrated entropy formula.

    Validated against LLM assessments.
    MAE = 0.172, Correlation = 0.71
    """
    base = 1.0

    # Effective corroboration (discounted by independence)
    # Key insight: 65% of sources are copies, only 35% are independent
    effective_corr = n_corr * independence_ratio

    # Calibrated weights from optimization
    corr_reduction = 0.49 * (effective_corr ** 0.30)
    contra_addition = 0.27 * (n_contra ** 0.31)

    entropy = base - corr_reduction + contra_addition
    return max(0.05, min(0.99, entropy))
```

### Why Independence Matters

```
100 outlets copying AP ≠ 100 independent sources
```

Empirical finding: **65% of news corroborations are copies**. Naive counting overestimates certainty by ~3x.

The formula discounts by `independence_ratio = 0.35` to account for this.

### Universal Claim Topology Definition

```python
CLAIM := {
    content: str,           # What is asserted
    entropy: float,         # Uncertainty (0=certain, max=unknown)

    supported_by: [CLAIM],  # Evidence for this claim
    contradicted_by: [CLAIM],

    abstraction_level: computed,  # Graph distance from ground
}

TOPOLOGY := Graph(nodes=CLAIM, edges=SUPPORTS|CONTRADICTS)

INDEX := {
    entropy: uncertainty measure,
    coherence: 1 - normalized_entropy,
    plausibility: P(claim | all evidence)
}
```

### Rigorous Example: Criminal Justice

```
GROUND CLAIMS (abstraction=0):
├── "Fingerprint on weapon" (from forensic report)
├── "A seen near scene at 9pm" (from witness)
├── "A had conflict with victim" (from texts)
├── "A's alibi: was at bar" (from A's statement)
├── "Bar receipt at 9:15pm" (from physical evidence)

PATTERN CLAIM (abstraction=1):
├── "Evidence suggests A was at scene"
│   entropy = f(fingerprint supports, alibi contradicts)
│
└── "A's alibi has partial support"
    entropy = f(receipt supports, but 15min gap)

HIGHER CLAIM (abstraction=2):
└── "A is guilty"
    entropy = f(all evidence, weighted by independence)

    Current state: HIGH ENTROPY (contested)
    - Fingerprint + motive: supports
    - Alibi + receipt: contradicts
    - Needs resolution
```

**The topology shows exactly why we're uncertain and what would resolve it.**

### Why This Is Universal

| Domain | Ground Claims | Pattern Claims | High Claims |
|--------|---------------|----------------|-------------|
| **Physics** | Measurements | Laws | Theories |
| **Medicine** | Symptoms, tests | Syndromes | Diagnoses |
| **Law** | Evidence | Arguments | Verdicts |
| **History** | Sources | Interpretations | Narratives |
| **News** | Reports | Events | Frames |
| **Science** | Observations | Hypotheses | Theories |

**The same topology. The same entropy. The same operation.**

---

## Part 3: Epistemic Metabolism

Metabolism is the recurring loop that keeps an event "alive" while evidence is flowing:

1. Ingest new artifacts → extract claims (with provenance).
2. Add/strengthen relations among claims (corroborates/contradicts/updates).
3. Recompute plausibility/posteriors given priors + relations.
4. Update event state (facets, timelines, contended sets, summaries).
5. Emit changes (UI updates, alerts, "what changed", questions).
6. Hibernate when stable; reactivate on new evidence or strong contradictions.

Metabolism is not "always generate a new summary"; it is "always keep belief state revisable and auditable".

### Complementary Intelligence

Human cognition evolved for **survival**, not **truth-seeking**. This creates systematic biases:

| Bias | What It Does | Why It Evolved | Epistemic Cost |
|------|--------------|----------------|----------------|
| **Confirmation bias** | Seek info that confirms belief | Faster decisions | Miss contradicting evidence |
| **Emotional reasoning** | Feelings override evidence | Threat response | Irrational conclusions |
| **Affirmation bias** | Prefer to agree | Tribal cohesion | Suppress valid dissent |
| **Recency bias** | Overweight recent info | Predator awareness | Discount stable evidence |

**Epistemic organisms compensate:**

| Human Deficiency | Epistemic Organism Design |
|------------------|---------------------------|
| Confirmation bias | Actively seek contradicting sources |
| Emotional reasoning | No emotions, only evidence weights |
| Affirmation bias | Designed to surface disagreement |
| Memory limits | Unlimited claim storage |

**Together**: Humans contribute sources, context, and judgment. Epistemic organisms track, weight, and surface. The combination produces better resolution than either alone.

### Core Commitments (Non-Negotiables)

1. **Claim-grounded**: every statement traces back to source artifacts (provenance is not optional).
2. **Explicit uncertainty**: "unknown" is an allowed output; confidence is computed and explainable.
3. **Contradiction surfacing**: disagreement is made legible (contested sets), not smoothed into a single voice.
4. **Non-monotonic update**: new evidence may demote prior conclusions without erasing history.
5. **Diversity-aware inference**: repetition is not corroboration; source independence matters.

### Facets and Branching

"One event" must remain navigable without collapsing disagreement into a single blob.

**Facets to Expose:**
- **Time slices**: cluster claims by event-time windows (change-points or simple clustering)
- **Metric tracks**: monotone-ish update chains for topic-keyed counters (deaths/injured/damage)
- **Hub entities**: top entities by support count/salience, grouped by role
- **Source diversity**: distinct publishers/types per facet; concentration flags
- **Contested sets**: explicit "live disagreements" per facet (not hidden)

**Branching Logic:**
Prefer facets by default; branch only when divergence is persistent and source-diverse.

Typical branch triggers:
- temporal divergence (distinct burst with low overlap)
- metric divergence that would pollute a single track
- a new hub entity that becomes its own coherent locus of claims

### Hardening Checklist

1. **Calibrate signals**: collect labeled sets; scale multi-signal weights; store thresholds as config.
2. **Numeric guardrails**: enforce monotone-ish update expectations for counters; treat large downward jumps as contradictions.
3. **Source diversity penalty**: down-weight clusters dominated by one publisher/domain.
4. **Evidence cards everywhere**: narratives must cite top-k claims and show why confidence moved.
5. **Change-triggered metabolism**: if a new claim strongly contradicts consensus, re-run immediately.

### Auditability: The Hard Requirement

Every material belief update should be replayable and inspectable. For each metabolism run, persist:
- priors used (and their versions)
- relation counts/types and the main contributing edges
- top-k claims by plausibility (with provenance)
- diversity stats (publisher/domain mix, similarity penalties)
- contradictions surfaced
- timestamps and triggering condition

---

## Success Metrics

The system succeeds when:

1. **Events converge** — entropy decreases over time for factual questions
2. **Contradictions surface** — disagreements are visible, not hidden
3. **Resolution is trackable** — users can see what we know vs. don't know
4. **Irreducible plurality is preserved** — normative questions stay open

```
φ⁰  = fully resolved (entropy ≈ 0)
φ±  = partially resolved (entropy moderate, stable plurality)
φΩ  = irreducible plurality (normative/evolving)
```

---

## References

- Shannon, C.E. "A Mathematical Theory of Communication" (1948)
- Jaynes, E.T. "Probability Theory: The Logic of Science" (2003)
- `25.theory.economics.md` — Incentive alignment & anti-gaming
- `26.theory.calibration.md` — Experimental validation
- `30.architecture.md` — Implementation

---

*The measure of success is not engagement, not votes, not popularity—it is entropy reduction. The same operation applies at all scales. Everything else follows.*
