# 30 — Architecture

> Building a breathing knowledge system: design philosophy, engine, and implementation.

---

## The Vision: Global Investigative Platform

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                                                                             │
│  THIS IS NOT NEWS. THIS IS COLLABORATIVE EPISTEMIC INVESTIGATION.           │
│                                                                             │
│  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐         │
│  │   WIRE FEEDS    │    │  OFFICIAL DOCS  │    │  USER WITNESSES │         │
│  │   AFP, Reuters  │    │  Gov statements │    │  On-the-ground  │         │
│  └────────┬────────┘    └────────┬────────┘    └────────┬────────┘         │
│           │                      │                      │                   │
│           └──────────────────────┼──────────────────────┘                   │
│                                  │                                          │
│                                  ▼                                          │
│           ┌──────────────────────────────────────────────┐                  │
│           │                                              │                  │
│           │              UNIVERSAL EPISTEMIC             │                  │
│           │                   ENGINE                     │                  │
│           │                                              │                  │
│           │  • Emerges events from claim topology        │                  │
│           │  • Tracks provenance chains                  │                  │
│           │  • Computes corroboration networks           │                  │
│           │  • Identifies contradictions                 │                  │
│           │  • Detects update sequences                  │                  │
│           │  • Grades epistemic confidence               │                  │
│           │  • Generates contribution opportunities      │                  │
│           │                                              │                  │
│           └──────────────────────┬───────────────────────┘                  │
│                                  │                                          │
│                                  ▼                                          │
│           ┌──────────────────────────────────────────────┐                  │
│           │                                              │                  │
│           │           EPISTEMIC NARRATIVE                │                  │
│           │                                              │                  │
│           │  Multi-voice, temporal, transparent,         │                  │
│           │  participatory, living investigation         │                  │
│           │                                              │                  │
│           └──────────────────────────────────────────────┘                  │
│                                                                             │
│  ANYONE can contribute evidence. EVERYTHING is attributed.                  │
│  UNCERTAINTY is visible. TRUTH emerges from the topology.                   │
│                                                                             │
│  This is Jaynes' probability theory as collaborative journalism.            │
│  This is Wikipedia meets investigative reporting meets crowdsourcing.       │
│  This is the global newsroom where evidence speaks.                         │
│                                                                             │
└─────────────────────────────────────────────────────────────────────────────┘
```

### What UEE Must Provide

| Traditional System | UEE Output |
|--------------------|------------|
| Event title | Event with all Jaynesian quantities |
| Summary paragraph | Claims organized by time + voice |
| Source list | Full provenance chains |
| — | Corroboration/contradiction graph |
| — | Value evolution chains |
| — | Uncertainty markers with bounties |
| — | Contribution hooks |

---

## Part 1: Design Philosophy

### The Wikipedia Problem

**Wikipedia** is humanity's greatest collaborative knowledge project, but it has fundamental limitations:

1. **Static by nature** - Articles freeze until someone manually updates them
2. **Source opacity** - Citations exist, but relationship to claims is loose
3. **Edit wars** - Conflicts resolved by moderator judgment, not evidence weight
4. **Single narrative** - One article per topic, erasing valid alternative perspectives
5. **Language silos** - Cross-language linking is manual and incomplete

### Our Approach: Beyond Wikipedia

We're building a knowledge base that **breathes**:

1. **Evidence-first** - Every fact anchored to source artifacts
2. **Claim-atomic** - Facts are atomic, not paragraphs - can be verified independently
3. **Graph-native** - Relationships are first-class citizens with confidence
4. **Multi-lingual by default** - Entities link across languages via canonical IDs
5. **Confidence evolves** - Trust rises with evidence, falls with age or contradictions
6. **Community + economics** - Disputes resolved by weighted voting
7. **Facts, not narratives** - System produces events; humans curate stories

### Core Principles

#### 1. Resource-First, Not Pipeline

```
Anti-pattern:  URL → Pipeline processes everything → Return result or error
Our pattern:   URL → Create stub → Return immediately → Workers enrich asynchronously
```

User gets response in < 100ms. Workers run independently.

#### 2. Confidence Lives in the Graph

```
confidence = f(semantic signals, structural signals, temporal factors, contradiction penalties)
```

Not just a model score, but graph-aware computation.

#### 3. Instant vs Lazy Discipline

| Tier | Latency | Allowed |
|------|---------|---------|
| Instant | < 500ms | DB/cache only |
| Lazy | Async | External APIs, LLMs, heavy computation |

#### 4. Facts Over Narratives

```
Article → Extract claims → Cluster into events → (Stop)
Narratives = community layer (not system-generated)
```

Claims are verifiable. Narratives are subjective.

---

## Part 2: Universal Epistemic Engine (UEE)

The UEE is the computational heart of HERE.news. It implements a **single fractal operation** that works at all scales:

```
compute_affinity(A, B) → score ∈ [0, 1]
```

### Multi-Signal Scoring

```python
affinity = (
    0.60 × semantic_similarity +    # Embedding cosine similarity
    0.25 × entity_overlap +          # Shared entities
    0.15 × entity_specificity        # IDF-weighted rare entity bonus
)
```

| Signal | Weight | Rationale |
|--------|--------|-----------|
| **Semantic** | 60% | Topical relatedness |
| **Entity Overlap** | 25% | Shared actors/places |
| **Entity Specificity** | 15% | Rare entities = stronger signal |

### Data Structures

```python
Claim:
    id, text, page_id, entity_ids, embedding, timestamp

Event:
    id, claim_ids, entity_surface, embedding_centroid
    mass, heat, entropy, coherence  # Jaynesian quantities
    sources, state  # LIVE | WARM | DORMANT | ARCHIVED
```

### Processing Pipeline

```
Phase 1: Claim Processing
    for each claim:
        score = compute_affinity(claim, event)
        if score >= 0.45: JOIN event
        else: SEED new event

Phase 2: Event Merge (Same Rules)
    for each event pair:
        if affinity >= 0.50: MERGE

Phase 3: Distillation
    Output: mass, claims, sources, entities, coherence, entropy
    Plus: what_we_know, what_differs, evolution
```

### Display Metrics

```python
display_score = mass × heat_factor × diversity_factor × (1 - entropy_penalty)
```

| Tier | Criteria | Use Case |
|------|----------|----------|
| **HEADLINE** | display > 100, sources ≥ 5 | Homepage feature |
| **SIGNIFICANT** | display > 50, sources ≥ 3 | Category pages |
| **EMERGING** | display > 20, high heat | Watchlist |
| **INTERNAL** | display < 20 | Not yet public |

### Independence-Weighted Corroboration

**The Copying Problem**: 65% of news "corroborations" are just copies.

```python
# Wrong: Naive voting
2000 sources > 50 sources → Berlin wins (incorrect)

# Right: Independence weighting
Berlin: 2000 sources × 0.01 independence = 20 effective
Paris:  50 sources × 0.80 independence = 40 effective
→ Paris wins
```

---

## Part 3: Event Formation

### Progressive Data Accumulation

```
DAY 1: Initial Event
├── 1 story, ~8 pages, ~35 claims, ~20 entities

WEEK 2: First Development
├── +3 pages, +15 claims, +5 entities
├── Stories linked via shared entities

WEEK 5: Major Development
├── +5 pages, +28 claims, +12 entities
└── Event aggregates: 2 stories, 13 pages, 63 claims, 32 entities
```

### Scale Patterns

| Scale | Duration | Stories | Claims | Entities |
|-------|----------|---------|--------|----------|
| **Micro** | Hours | 1-2 | 20-40 | 5-10 |
| **Meso** | Weeks | 2-5 | 50-100 | 30-50 |
| **Macro** | Months | 100+ | 10,000+ | 500+ |

### Key Mechanisms

1. **Temporal Clustering**: Claims with `event_time` within 7 days + shared entities = same event
2. **Entity-Based Linking**: Graph traversal via shared canonical entities
3. **Credibility Propagation**: Page → Claim → Timeline credibility
4. **Incremental Updates**: New claims refine existing events

---

## Part 4: Technical Stack

### System Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│                              FRONTEND                                    │
│                    React 18 + TypeScript + Vite                         │
└─────────────────────────────────┬───────────────────────────────────────┘
                                  │ HTTP/REST
┌─────────────────────────────────▼───────────────────────────────────────┐
│                           BACKEND API                                    │
│                         FastAPI (Python 3.12)                            │
└──────────┬──────────────────┬──────────────────────┬────────────────────┘
           │                  │                      │
           ▼                  ▼                      ▼
┌──────────────────┐  ┌───────────────┐  ┌─────────────────────────────────┐
│    PostgreSQL    │  │    Neo4j      │  │          Redis                  │
│    (pgvector)    │  │  (Graph DB)   │  │      (Job Queue)                │
└──────────────────┘  └───────────────┘  └───────┬─────────┬───────┬──────┘
                                                 │         │       │
                      ┌──────────────────────────┼─────────┼───────┼──────┐
                      │                   WORKERS                          │
                      │  Extraction (×2) │ Knowledge (×2) │ Event (×1)    │
                      └───────────────────────────────────────────────────┘
```

### Component Summary

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Frontend** | React 18 + Vite | SPA interface |
| **API** | FastAPI | REST endpoints |
| **PostgreSQL** | pgvector | Content + embeddings |
| **Neo4j** | 5.15 | Entity graph |
| **Redis** | 7-alpine | Job queues |
| **Workers** | Python async | Pipeline processing |

### Pipeline Flow

```
URL Submitted → status: stub (< 100ms)
       ↓
Extraction Worker → status: extracted
       ↓
Knowledge Worker → LLM extraction, Wikidata, deduplication
       ↓ → status: knowledge_complete
Event Worker → Multi-signal scoring, metabolism, narrative
       ↓
status: event_complete ← Ready for display
```

### Multi-Signal Claim Scoring

| Signal | Weight | Description |
|--------|--------|-------------|
| Entity overlap | 0.20 | Shared entities |
| Temporal proximity | 0.20 | Time from event |
| Reference detection | 0.25 | Same incident |
| Semantic similarity | 0.15 | Embedding cosine |
| Spatial overlap | 0.10 | Location entities |
| Causal keywords | 0.10 | Causal patterns |

---

## Part 5: Scaling

### Connection Pooling

Current architecture:
- Backend: 1 pool, max 10 connections
- Each Worker: 1 pool, max 3 connections

**Problem**: At ~10 workers each, you hit PostgreSQL limit (100).

### Production Solution: PgBouncer

```yaml
pgbouncer:
  environment:
    - POOL_MODE=transaction
    - MAX_CLIENT_CONN=500
    - DEFAULT_POOL_SIZE=25
```

Result: 500 workers share 25 PostgreSQL connections.

### Performance Targets

| Metric | Target |
|--------|--------|
| Instant tier | < 500ms p95 |
| URL extraction | < 5s p95 |
| Event formation | < 30s p95 |
| Entity duplication | < 10% |
| Wikidata linkage | > 85% |

---

## Best Practices

1. **Idempotency**: Same URL → same resource, updated metadata
2. **Graceful Degradation**: Worker fails → others continue
3. **Observable System**: Logs, confidence breakdowns, health metrics
4. **Event-Driven**: Workers communicate via queue only
5. **Data Provenance**: Every entity tracks extraction method, sources
6. **Domain Models**: NEVER raw dicts from DB in API responses

---

## References

- `20.theory.md` — Epistemic foundation
- `33.arch.data-structures.md` — Schema specification
- `34.arch.diagrams.md` — Visual architecture

---

*A breathing knowledge base: rising and falling with the tide of evidence, adapting to reality faster than any human-edited encyclopedia.*
