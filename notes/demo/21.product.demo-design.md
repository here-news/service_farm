# 21 — Demo Design & Implementation

## Overview

This demo proves the **resource-first breathing architecture** end-to-end with a simplified, containerized implementation. It successfully demonstrates:

1. **Instant best shot response** (< 100ms) with background worker enrichment
2. **Independent, scalable workers** (extraction → semantic → event formation)
3. **Real-time event formation and evolution** visible in UI
4. **LLM-powered event synthesis** generating structured events from claims
5. **Progressive enhancement** from stub → complete artifact

## Architecture Principles

### Resource-First Pattern

The core pattern implemented:

```
POST /artifacts/draft?url=https://example.com/article
↓
< 100ms response with best shot (stub or existing data)
↓
Background: commission_workers() → Redis queues
↓
Workers process independently: extraction → semantic → event
↓
Frontend polls /artifacts/draft/{id} for updates
```

**Two scenarios:**
- **Existing URL**: Instant best shot from database (entities, events, confidence)
- **New URL**: Create stub, commission workers, return low confidence (0.3)

### Worker Independence

**Critical requirement**: "that's exact what we hate before" - each worker must be independently scalable.

Implementation:
- **3 separate containers**: demo-worker-extraction, demo-worker-semantic, demo-worker-event
- **3 separate Python files**: worker_extraction.py, worker_semantic.py, worker_event.py
- **Independent restart policies**: Each worker can fail/restart without affecting others
- **Scalable**: `docker-compose up --scale demo-worker-extraction=3`

**Redis queue architecture:**
```
queue:extraction:high    → ExtractionWorker
queue:semantic:normal    → SemanticWorker
queue:event:normal       → EventWorker
```

Workers auto-trigger next stage:
- Extraction completes → queues semantic worker
- Semantic completes → queues event worker

### Three-Stage Pipeline

#### 1. ExtractionWorker
- Fetches URL content using urllib.request (with User-Agent header)
- Extracts title, text using trafilatura
- Updates page status: stub → extracted
- Queues SemanticWorker

**Key fix**: trafilatura.fetch_url() failed on news sites, switched to urllib.request with custom User-Agent.

#### 2. SemanticWorker
- Uses OpenAI GPT-4o-mini to extract entities and claims
- Stores entities in PostgreSQL with canonical names
- Extracts factual claims with optional event_time
- Updates page status: extracted → entities_extracted
- Queues EventWorker

**LLM Prompt structure:**
```python
"""Extract entities and claims from this article.
Return JSON with:
{"entities": [{"name": "Hong Kong", "type": "GPE", "confidence": 0.95}],
 "claims": [{"text": "Fire killed 44 people", "confidence": 0.9,
             "event_time": "2025-11-26T09:30:00Z"}]}
Only factual claims. Max 10 entities, max 5 claims."""
```

#### 3. EventWorker
- Matches claims to existing events (2+ shared entities + LLM matching)
- OR synthesizes new event using LLM
- Updates event summaries progressively as articles join
- Updates page status: entities_extracted → complete

**Event synthesis prompt (critical for proper titles):**
```python
"""Create a structured event description with:
1. Event title: Describe the CORE EVENT as a noun phrase, NOT a headline
   - Good: "DOJ Prosecution of James Comey for False Statements"
   - Bad: "James Comey Seeks Dismissal of DOJ Case" (this is a headline)
2. Summary (2-3 sentences) explaining what the event is about
3. Event type (legal_proceeding, investigation, incident, etc.)
4. Location (if mentioned)
5. Timeframe (if mentioned, ISO format)"""
```

**Result**: Successfully generated proper event titles and merged 4 articles into main event.

## Database Schema (Simplified)

**Design decision**: Use PostgreSQL-only for demo simplicity. Production would use Neo4j for entity graph.

Key tables:
- **pages**: Artifacts with content, status progression, language
- **entities**: Canonical entities with type, language, confidence
- **page_entities**: Many-to-many with mention counts
- **claims**: Extracted factual claims with optional event_time
- **events**: Structured events with type, location, timeframe
- **page_events**: Many-to-many linking pages to events
- **event_entities**: Many-to-many linking events to entities

**Status progression:**
```
stub → extracting → extracted → entities_extracted → complete
```

**Confidence calculation:**
```python
def calculate_confidence(page, entities, events):
    conf = 0.0
    if page.get('content_text'): conf += 0.3
    if page.get('title'): conf += 0.2
    if entities: conf += 0.3
    if events: conf += 0.2
    return min(1.0, conf)
```

## Docker Architecture

### Container Composition

7 independent services:

1. **demo-postgres**: PostgreSQL 15 on port 5433 (isolated from production)
2. **demo-redis**: Redis 7 for worker queues
3. **demo-backend**: FastAPI server on port 8001
4. **demo-worker-extraction**: Extraction worker (scalable)
5. **demo-worker-semantic**: Semantic analysis worker (scalable)
6. **demo-worker-event**: Event formation worker (scalable)
7. **demo-frontend**: Simple Python HTTP server on port 8002

### Key Configuration

**Connection pooling** (critical to prevent "too many clients" errors):
```yaml
# Backend
pool = await asyncpg.create_pool(..., min_size=2, max_size=5)

# Each worker
pool = await asyncpg.create_pool(..., min_size=1, max_size=3)
```

**Python unbuffering** (for visible logs):
```yaml
command: python -u worker_extraction.py
environment:
  - PYTHONUNBUFFERED=1
```

**Health checks**:
```yaml
healthcheck:
  test: ["CMD", "pg_isready", "-U", "demo_user"]
  interval: 10s
```

### Debugging Tools in Containers

**PRAISE**: Unlike previous service_farm which "constantly fail to find such cli", this demo includes debugging tools directly in containers:

```bash
# PostgreSQL debugging
docker exec -it demo-postgres psql -U demo_user -d demo_phi_here

# Redis debugging
docker exec -it demo-redis redis-cli

# Worker logs
docker logs demo-worker-extraction -f
docker logs demo-worker-semantic -f
docker logs demo-worker-event -f
```

This design choice dramatically improves debugging experience and reduces "CLI not found" issues.

## Frontend Design

### Two-Column Layout with Events Sidebar

**User requirement**: "it's better to have an event list at side bar in UI, and showing it's changes"

Implementation:
- **Left sidebar**: Real-time events list with article counts, timestamps
- **Main content**: URL input + article processing status
- **Auto-refresh**: Events refresh every 5s, article status polls every 2s

**Events display:**
```javascript
async function loadEvents() {
    const response = await fetch(`${API_BASE}/events`);
    const data = await response.json();
    allEvents = data.events || [];
    renderEventsList();
}

setInterval(loadEvents, 5000);  // Refresh every 5 seconds
```

**Visual feedback:**
- Article count badges
- Relative timestamps ("2m ago", "1h ago")
- Event type labels
- Confidence indicators

### Live Polling Pattern

Frontend polls for updates:
```javascript
async function pollStatus(artifactId) {
    while (polling) {
        const response = await fetch(`${API_BASE}/artifacts/draft/${artifactId}`);
        const data = await response.json();

        if (data.status === 'complete') {
            polling = false;
        }

        await new Promise(r => setTimeout(r, 2000));  // Poll every 2s
    }
}
```

## Demonstration Results

### Successful Processing Examples

**Wikipedia article (https://en.wikipedia.org/wiki/Breathing):**
- 3930 words extracted
- 5 entities: Breathing (CONCEPT), Lungs (CONCEPT), Oxygen (CONCEPT), Carbon Dioxide (CONCEPT), Respiratory System (CONCEPT)
- 3 claims extracted
- 1 event created: "Breathing as a Life Process"

**James Comey DOJ case (4 articles from ABC, NBC, CNN, Guardian):**
- Main event: "DOJ Prosecution of James Comey for False Statements to Congress"
- Successfully merged 4 articles with coherent progressive summary
- Event type: legal_proceeding
- Location: United States
- Proper noun phrase title (not headline)

**LLM-powered event synthesis examples:**

First article creates event:
```json
{
  "title": "DOJ Prosecution of James Comey for False Statements to Congress",
  "summary": "James Comey faces federal charges for allegedly making false statements to Congress during his tenure as FBI Director. The case involves accusations of providing misleading testimony regarding the FBI's investigation into Hillary Clinton's emails.",
  "event_type": "legal_proceeding",
  "location": "United States"
}
```

Second article matches and updates summary:
```json
{
  "title": "DOJ Prosecution of James Comey for False Statements to Congress",
  "summary": "James Comey faces federal charges for allegedly making false statements to Congress. Federal prosecutors have urged the judge to reject Comey's claim of vindictive prosecution, arguing the charges are based on evidence. Comey's legal team is seeking dismissal of the case.",
  "article_count": 2
}
```

## What Was Successfully Proven

✅ **Resource-first architecture works**: Instant stub response with background enrichment
✅ **Independent workers scale**: Can run multiple extraction workers in parallel
✅ **LLM event synthesis**: Generates structured, coherent event descriptions
✅ **Event matching and merging**: Successfully merged 4 articles into single event
✅ **Real-time UI updates**: Events sidebar shows formation and evolution
✅ **Progressive enhancement**: Confidence increases as workers complete
✅ **Isolated environment**: Port 5433 PostgreSQL, no production contamination
✅ **Debugging tools in containers**: psql, redis-cli accessible without CLI installation issues

## Known Issues and Limitations

### 1. Connection Pool Exhaustion (Ongoing)

**Problem**: PostgreSQL "too many clients already" errors occur intermittently

**Attempted fixes:**
- Added connection pool limits (backend: 2-5, workers: 1-3)
- Proper pool.close() in endpoints

**Root cause**: `get_entities_for_page()` and `get_events_for_page()` may not properly close connections in all code paths

**Workaround**: Restart backend when errors occur

### 2. Event Granularity (Open Research Question)

**Problem**: Multiple related events created when they should merge into parent event

**Example**: James Comey case created 3 separate events:
1. "DOJ Prosecution of James Comey for False Statements to Congress" (4 articles)
2. "Dismissal Motion in James Comey False Statements Case" (1 article)
3. "Charges Against James Comey and Lindsey Halligan's Appointment" (1 article)

**User observation**: "apparently they are all under one bigger event"

**Decision**: "we can pause for event evolution methods to revisit" - this is a deeper research problem

**Open questions for future exploration:**
- When to merge events vs create sub-events?
- How to model event hierarchies (parent/child relationships)?
- What defines event granularity levels?
- Should we have event types: atomic, compound, meta-event?
- How to handle event evolution over time (same event, new phase)?

### 3. Entity Storage Simplification

**Current**: Entities stored in PostgreSQL for demo simplicity
**Production**: Should use Neo4j for entity graph with relationships

**Trade-off rationale**: Demo focuses on proving worker pipeline and event formation. Entity graph complexity deferred to production implementation.

## Comparison to Previous service_farm

### Improvements

1. **CLI tools in containers**: No more "constantly fail to find such cli" - psql and neo4j-shell included
2. **Independent workers**: Not single-process with asyncio.gather() which "we hate before"
3. **Simplified schema**: Clear separation of concerns, easier to understand
4. **Better logging**: PYTHONUNBUFFERED=1 and python -u for visible worker output
5. **Connection pooling**: Explicit limits prevent resource exhaustion
6. **Events UI**: Real-time visualization of event formation

### Lessons Applied

- Don't create single-process multi-worker systems
- Include debugging tools in containers from day 1
- Use connection pooling with explicit limits
- Make worker logs visible (unbuffered output)
- Design for independent scaling from the start

## Testing Procedure

### Manual Testing

URLs tested:
1. https://www.politico.com/news/2025/11/01/james-comey-letitia-james-criminal-cases-00632037
2. https://en.wikipedia.org/wiki/Breathing
3. https://abcnews.go.com/US/prosecutors-urge-judge-reject-fbi-director-james-comeys/story?id=127148682
4. https://www.cnn.com/2025/11/03/politics/trump-social-media-james-comey-letitia-james
5. https://www.nbcnews.com/politics/justice-department/judge-accuses-prosecutors-comey-case-taking-indict-first-investigate-l-rcna242222
6. https://www.cnn.com/2025/11/13/politics/takeaways-lindsey-halligan-comey-james-hearing
7. https://www.theguardian.com/us-news/2025/nov/13/james-comey-letitia-james-challenge-trump-charges
8. https://apnews.com/article/comey-letitia-james-justice-department-ff6435d7bdbcc9e9495844899c7e2c4e

### Verification Steps

1. Start demo: `./start_demo.sh`
2. Open http://localhost:8002
3. Submit article URL
4. Observe instant stub response (< 100ms)
5. Watch status progression: stub → extracting → extracted → entities_extracted → complete
6. Verify events sidebar updates
7. Submit related article
8. Verify event matching and summary update
9. Check worker logs: `docker logs demo-worker-extraction -f`
10. Debug database: `docker exec -it demo-postgres psql -U demo_user -d demo_phi_here`

### Example Queries for Debugging

```sql
-- Check page status
SELECT id, url, title, status, created_at FROM pages ORDER BY created_at DESC;

-- Check entities extracted
SELECT e.canonical_name, e.entity_type, COUNT(pe.page_id) as page_count
FROM entities e
JOIN page_entities pe ON e.id = pe.entity_id
GROUP BY e.canonical_name, e.entity_type
ORDER BY page_count DESC;

-- Check events with article counts
SELECT e.title, e.event_type, e.location, COUNT(pe.page_id) as article_count
FROM events e
LEFT JOIN page_events pe ON e.id = pe.event_id
GROUP BY e.id
ORDER BY article_count DESC;

-- Check claims for a page
SELECT text, confidence, event_time FROM claims WHERE page_id = '<uuid>';
```

## Critical Implementation Details

### URL Fetching (worker_extraction.py:27-35)

```python
# CRITICAL: trafilatura.fetch_url() fails on news sites
# Use urllib.request with User-Agent instead
import urllib.request
req = urllib.request.Request(
    url,
    headers={'User-Agent': 'Mozilla/5.0 (compatible; BreathingKB/1.0)'}
)
with urllib.request.urlopen(req, timeout=30) as response:
    downloaded = response.read().decode('utf-8', errors='ignore')
```

### Datetime Parsing (worker_semantic.py:95-101, worker_event.py:143-149)

```python
# CRITICAL: LLM returns ISO strings, PostgreSQL needs datetime objects
event_time = claim.get('event_time')
if event_time and isinstance(event_time, str):
    from dateutil import parser as dateutil_parser
    try:
        event_time = dateutil_parser.parse(event_time)
    except:
        event_time = None
```

### Event Title Generation (worker_event.py:115-125)

```python
# CRITICAL: Must specify noun phrase, not headline
prompt = """
1. Event title: Describe the CORE EVENT as a noun phrase, NOT a headline
   - Good: "DOJ Prosecution of James Comey for False Statements"
   - Bad: "James Comey Seeks Dismissal of DOJ Case" (this is a headline)
"""
```

### Docker Frontend Cache (start_demo.sh)

```bash
# CRITICAL: Docker may cache frontend files
# Force copy after rebuild
docker cp frontend/index.html demo-frontend:/app/index.html 2>/dev/null || true
```

## Future Research Directions

### Event Evolution Mechanisms

**Question**: How should events evolve as new articles arrive?

**Current behavior**:
- LLM checks if claims match existing event
- If yes: Update event summary progressively
- If no: Create new event

**Needed research:**
- Event hierarchy modeling (parent/child, super-event/sub-event)
- Event merging criteria (when to combine separate events)
- Event splitting criteria (when single event becomes multiple)
- Temporal event relationships (precedes, follows, concurrent)
- Event granularity levels (atomic vs compound vs meta-events)

**Example use case**: James Comey case
- Should "Dismissal Motion" be separate event or sub-event of "DOJ Prosecution"?
- How to represent progression: Indictment → Motion to Dismiss → Hearing → Verdict?
- When does one legal case split into multiple events?

### Entity Graph Integration

**Question**: How to integrate Neo4j entity graph with PostgreSQL event store?

**Considerations:**
- Keep events in PostgreSQL (time-series, structured queries)
- Keep entities in Neo4j (graph relationships, entity resolution)
- Sync mechanism between systems
- Query patterns for "events involving entity X"
- Entity evolution tracking (aliases, merges, splits)

### LLM Prompt Optimization

**Question**: Can we improve event synthesis and matching accuracy?

**Areas to explore:**
- Few-shot examples in prompts
- Chain-of-thought reasoning for event matching
- Structured output with JSON schema validation
- Multi-step event synthesis (claims → temporal structure → event)
- Cross-document entity coreference

## Deployment Notes

### Starting the Demo

```bash
cd /media/im3/plus/lab4/re_news/service_farm/demo
./start_demo.sh
```

This script:
1. Stops existing containers
2. Rebuilds all images
3. Starts all 7 services
4. Waits for PostgreSQL health check
5. Copies latest frontend files (bypass Docker cache)
6. Opens http://localhost:8002

### Stopping the Demo

```bash
./stop_demo.sh
```

### Debugging Individual Workers

```bash
# Watch extraction worker
docker logs demo-worker-extraction -f

# Watch semantic worker
docker logs demo-worker-semantic -f

# Watch event worker
docker logs demo-worker-event -f

# Check Redis queues
docker exec -it demo-redis redis-cli
> LLEN queue:extraction:high
> LLEN queue:semantic:normal
> LLEN queue:event:normal
```

### Inspecting Database State

```bash
# Connect to PostgreSQL
docker exec -it demo-postgres psql -U demo_user -d demo_phi_here

# Check page processing status
SELECT id, LEFT(url, 50) as url, status, created_at FROM pages ORDER BY created_at DESC;

# Check event formation
SELECT title, event_type, (SELECT COUNT(*) FROM page_events WHERE event_id = events.id) as articles
FROM events ORDER BY updated_at DESC;
```

## Conclusion

This demo successfully proves the breathing knowledge base architecture with:

- **Instant resource-first responses** that improve over time
- **Independent, scalable worker pipeline** that processes in stages
- **LLM-powered event formation** that creates coherent events from claims
- **Real-time UI** showing events forming and evolving
- **Excellent debugging experience** with CLI tools in containers

**Key achievement**: Demonstrated that the architecture works end-to-end with real news articles, forming events that accurately represent the underlying stories.

**Next steps**:
1. Address connection pool exhaustion
2. Research event evolution mechanisms (merging, hierarchies, granularity)
3. Integrate Neo4j for entity graph
4. Optimize LLM prompts for better event matching
5. Scale to larger article volumes

This demo provides a solid foundation for building the full production system while proving the core architectural concepts.
